{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook for [this post](https://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path      = \"/home1/dataset/mnist/mnist/\"\n",
    "test_labels_file  = \"test-labels.csv\"\n",
    "train_labels_file = \"train-labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    return int(label)\n",
    "\n",
    "def read_label_file(file):\n",
    "    f = open(file, \"r\")\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        filepath, label = line.split(\",\")\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(encode_label(label))\n",
    "    return filepaths, labels\n",
    "\n",
    "# reading labels and file path\n",
    "train_filepaths, train_labels = read_label_file(dataset_path + train_labels_file)\n",
    "test_filepaths, test_labels = read_label_file(dataset_path + test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform relative path into full path\n",
    "train_filepaths = [ dataset_path + fp for fp in train_filepaths]\n",
    "test_filepaths = [ dataset_path + fp for fp in test_filepaths]\n",
    "\n",
    "# for this example we will create or own test partition\n",
    "all_filepaths = train_filepaths + test_filepaths\n",
    "all_labels = train_labels + test_labels\n",
    "\n",
    "# we limit the number of files to 20 to make the output more clear!\n",
    "all_filepaths = all_filepaths[:20]\n",
    "all_labels = all_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home1/dataset/mnist/mnist/train-images/0.jpg', '/home1/dataset/mnist/mnist/train-images/1.jpg', '/home1/dataset/mnist/mnist/train-images/2.jpg', '/home1/dataset/mnist/mnist/train-images/3.jpg', '/home1/dataset/mnist/mnist/train-images/4.jpg', '/home1/dataset/mnist/mnist/train-images/5.jpg', '/home1/dataset/mnist/mnist/train-images/6.jpg', '/home1/dataset/mnist/mnist/train-images/7.jpg', '/home1/dataset/mnist/mnist/train-images/8.jpg', '/home1/dataset/mnist/mnist/train-images/9.jpg', '/home1/dataset/mnist/mnist/train-images/10.jpg', '/home1/dataset/mnist/mnist/train-images/11.jpg', '/home1/dataset/mnist/mnist/train-images/12.jpg', '/home1/dataset/mnist/mnist/train-images/13.jpg', '/home1/dataset/mnist/mnist/train-images/14.jpg', '/home1/dataset/mnist/mnist/train-images/15.jpg', '/home1/dataset/mnist/mnist/train-images/16.jpg', '/home1/dataset/mnist/mnist/train-images/17.jpg', '/home1/dataset/mnist/mnist/train-images/18.jpg', '/home1/dataset/mnist/mnist/train-images/19.jpg']\n",
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print all_filepaths\n",
    "print all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "# convert string into tensors\n",
    "all_images = ops.convert_to_tensor(all_filepaths, dtype=dtypes.string)\n",
    "all_labels = ops.convert_to_tensor(all_labels, dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(20,), dtype=string)\n",
      "Tensor(\"Const_1:0\", shape=(20,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print all_images\n",
    "print all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a partition vector\n",
    "import random\n",
    "import tensorflow as tf\n",
    "test_set_size = 5\n",
    "partitions = [0] * len(all_filepaths)\n",
    "partitions[:test_set_size] = [1] * test_set_size\n",
    "# random.shuffle(partitions)\n",
    "\n",
    "# partition our data into a test and train set according to our partition vector\n",
    "train_images, test_images = tf.dynamic_partition(all_images, partitions, 2)\n",
    "train_labels, test_labels = tf.dynamic_partition(all_labels, partitions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DynamicPartition:0\", shape=(?,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create input queues\n",
    "NUM_CHANNELS=1\n",
    "train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [train_images, train_labels],\n",
    "                                    shuffle=False)\n",
    "test_input_queue = tf.train.slice_input_producer(\n",
    "                                    [test_images, test_labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "# process path and string tensor into an image and a label\n",
    "file_content = tf.read_file(train_input_queue[0])\n",
    "train_image = tf.image.decode_jpeg(file_content, channels=NUM_CHANNELS)\n",
    "train_label = train_input_queue[1]\n",
    "\n",
    "file_content = tf.read_file(test_input_queue[0])\n",
    "test_image = tf.image.decode_jpeg(file_content, channels=NUM_CHANNELS)\n",
    "test_label = test_input_queue[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define tensor shape\n",
    "IMAGE_HEIGHT=28\n",
    "IMAGE_WIDTH=28\n",
    "BATCH_SIZE=5\n",
    "train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "\n",
    "\n",
    "# collect batches of images before processing\n",
    "train_image_batch, train_label_batch = tf.train.batch(\n",
    "                                    [train_image, train_label],\n",
    "                                    batch_size=BATCH_SIZE\n",
    "                                    #,num_threads=1\n",
    "                                    )\n",
    "test_image_batch, test_label_batch = tf.train.batch(\n",
    "                                    [test_image, test_label],\n",
    "                                    batch_size=BATCH_SIZE\n",
    "                                    #,num_threads=1\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-3649f7368e65>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "from the train set:\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "[7 2 8 6 9]\n",
      "[2 1 3 1 4]\n",
      "[3 5 3 6 1]\n",
      "from the test set:\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n",
      "[5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  \n",
    "    # initialize the variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    print \"from the train set:\"\n",
    "    for i in range(20):\n",
    "        print sess.run(train_label_batch)\n",
    "\n",
    "    print \"from the test set:\"\n",
    "    for i in range(10):\n",
    "        print sess.run(test_label_batch)\n",
    "\n",
    "    # stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
